import torch
import torchvision.transforms as T
import torchvision.models.video as models
import cv2
from PIL import Image
import numpy as np
import logging
from typing import List

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å R(2+1)D —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
weights = models.R2Plus1D_18_Weights.DEFAULT
model = models.r2plus1d_18(weights=weights)
model.eval()

# ‚úÖ –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
id2label = weights.meta["categories"]

# ‚úÖ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
preprocess = T.Compose(
    [
        T.Resize((112, 112)),
        T.ToTensor(),
        T.Normalize(mean=[0.45, 0.45, 0.45], std=[0.225, 0.225, 0.225]),
    ]
)


def extract_clip(
    video_path: str, num_frames: int = 16, skip: int = 4
) -> List[np.ndarray]:
    cap = cv2.VideoCapture(video_path)
    frames = []
    total = 0

    if not cap.isOpened():
        raise ValueError(f"‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if total % skip == 0:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        total += 1
        if len(frames) == num_frames:
            break

    cap.release()

    if len(frames) < num_frames:
        logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–¥—Ä–æ–≤: {len(frames)} / {num_frames}")
        frames += [frames[-1]] * (num_frames - len(frames))

    return frames


def classify_action(video_path: str) -> str:
    try:
        frames = extract_clip(video_path)
        return classify_action_from_clip(frames)
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ classify_action: {e}")
        return "unknown"


def classify_action_from_clip(frames: List[np.ndarray]) -> str:
    try:
        processed = [preprocess(Image.fromarray(f)) for f in frames]
        clip_tensor = (
            torch.stack(processed).permute(1, 0, 2, 3).unsqueeze(0)
        )  # (1, 3, T, H, W)

        with torch.no_grad():
            outputs = model(clip_tensor)
            predicted = outputs.argmax(dim=1).item()
            label = id2label[predicted]

        logger.info(f"üé¨ –ö–ª–∞—Å—Å –¥–µ–π—Å—Ç–≤–∏—è: {label}")
        return label

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ classify_action_from_clip: {e}")
        return "unknown"
