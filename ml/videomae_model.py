import torch
import torchvision.transforms as T
import cv2
from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification
import numpy as np
import os
import logging

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")
model = VideoMAEForVideoClassification.from_pretrained("MCG-NJU/videomae-base")
model.eval()

# ‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∫–∞–¥—Ä–æ–≤
transform = T.Compose([T.ToPILImage(), T.Resize((224, 224)), T.ToTensor()])


def load_frames(video_path: str, num_frames: int = 16, skip: int = 5) -> torch.Tensor:
    cap = cv2.VideoCapture(video_path)
    frames = []
    total = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if total % skip == 0:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(transform(frame))
        total += 1
        if len(frames) == num_frames:
            break
    cap.release()

    if len(frames) < num_frames:
        logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–∞–¥—Ä–æ–≤: {len(frames)} –≤–º–µ—Å—Ç–æ {num_frames}")
        # –î–æ–ø–æ–ª–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–º –∫–∞–¥—Ä–æ–º
        while len(frames) < num_frames:
            frames.append(frames[-1])

    return torch.stack(frames).unsqueeze(0)  # shape: (1, 16, 3, 224, 224)


def predict_video(video_path: str) -> str:
    logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ: {video_path}")
    inputs = load_frames(video_path)
    with torch.no_grad():
        outputs = model(pixel_values=inputs)
        predicted_class = outputs.logits.argmax(-1).item()
        label = model.config.id2label[predicted_class]
    logger.info(f"üéØ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {label}")
    return label
